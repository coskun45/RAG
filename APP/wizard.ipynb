{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SeiteAnzahl_documents_verbraucher_information: 114\n",
      "SeiteAnzahl_knowledgeBase: 114\n"
     ]
    }
   ],
   "source": [
    "loader3 = PyPDFLoader(\"../knowledgeBase/verbraucher_information.pdf\")#path für Knowledge Base\n",
    "documents_verbraucher_information = loader3.load_and_split()\n",
    "print(f\"SeiteAnzahl_documents_verbraucher_information: {len(documents_verbraucher_information)}\")\n",
    "knowledgeBase=documents_verbraucher_information\n",
    "print(f\"SeiteAnzahl_knowledgeBase: {len(knowledgeBase)}\")\n",
    "#print(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "texts_1024 = text_splitter.split_documents(knowledgeBase)\n",
    "print(len(texts_1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0005.206 Stand 10.2023AUTO / KLASSIK-GARANT 2.0\n",
      "ALLGEMEINE INFORMATIONEN\n",
      "VERSICHERUNGSBEDINGUNGEN\n",
      "MERKBLATT ZUR ANZEIGEPFLICHTVERLETZUNG\n",
      "DATENSCHUTZHINWEISE\n",
      "STAND 10/2023VERBRAUCHERINFORMATION \n",
      "NUMMER KN1023 PC\n"
     ]
    }
   ],
   "source": [
    "print(texts_1024[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ecoskun\\Desktop\\BachelorarbeitAbgabe\\bachelorarbeit_eyuep_coskun_3594788\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model_deutsche_telekom = HuggingFaceEmbeddings(model_name=\"deutsche-telekom/gbert-large-paraphrase-cosine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speicher Embeddings in db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"chroma_db_wizard\"\n",
    "db_path = os.path.join(db_name)\n",
    "if os.path.exists(db_path):\n",
    "    db=Chroma(persist_directory=\"chroma_db_wizard\", embedding_function=model_deutsche_telekom)# wenn db schon erstellt wurde,load db\n",
    "else:\n",
    "    db = Chroma.from_documents(texts_1024, model_deutsche_telekom, persist_directory=\"chroma_db_wizard\")# erstellt vektor datenbank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wizard-LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"C:/Users/ecoskun/AppData/Local/nomic.ai/GPT4All/wizardlm-13b-v1.2.Q4_0.gguf\"#local path für LLM\n",
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm= GPT4All(model=model_path, callbacks=callbacks, verbose=True,n_threads=16, temp=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt template für RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template='''\n",
    "Du bist ein hilfreicher Assistent. Für die folgende Aufgabe stehen dir zwischen den tags BEGININPUT und ENDINPUT Kontext zur Verfügung. Die eigentliche Frage ist zwischen BEGININSTRUCTION und ENDINCSTRUCTION zu finden. Beantworten Sie die folgende Frage nur basierend auf dem angegebenen context auf Deutsch. Sollten diese keine Antwort enthalten, antworte, dass auf Basis der gegebenen Informationen keine Antwort möglich ist! \n",
    "USER: \n",
    "BEGININPUT{context}ENDINPUT\n",
    "BEGININSTRUCTION {question} ENDINSTRUCTION \n",
    "ASSISTANT:\n",
    "'''\n",
    "PROMPT=PromptTemplate(\n",
    "    template=prompt_template,input_variables=[\"context\",\"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_type_kwargs={\"prompt\":PROMPT}\n",
    "qa_RAG_chain =RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    verbose=False,\n",
    "    chain_type_kwargs=chain_type_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure-LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "template = \"Du bist ein hilfreicher Assistent. Antworte Frage auf Deutsch USER: {prompt} ASSISTANT:\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=model_path, callbacks=callbacks, verbose=True)\n",
    "llm = GPT4All(model=model_path, backend=\"gptj\", callbacks=callbacks, verbose=True, max_tokens=8092)\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "try:\n",
    "    with open('../test/testDaten/HannoverscheVerscicherung.json', 'r') as datei:\n",
    "        json_data = json.load(datei)\n",
    "except Exception as errror:\n",
    "    print(f'errror: {errror}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erstellt Json Ergebnis Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"experiment_parameter\": {\n",
    "        \"LLM_model\": \"GPT4All/wizardlm-13b-v1.2.Q4_0\",\n",
    "        \"Embedding_model\": \"deutsche-telekom/gbert-large-paraphrase-cosine\",\n",
    "        \"chunk_size\": \"1024\",\n",
    "        \"chunk_overlap\": 64,\n",
    "        \"LLM_temperatur\": 0.5,\n",
    "        \"search_type\": \"similarity\",\n",
    "        \"search_kwargs\": \"3\",\n",
    "        \"chain_type\": \"stuff\"\n",
    "        },\n",
    "        \"wizard_ergebnisse\": []\n",
    "        }\n",
    "with open(\"wizard_antworten2.json\", \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speichert generierte Pure-LLM und RAG Antworten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(new_data, filename='wizard_antworten.json'):\n",
    "    with open(filename,'r+') as file:\n",
    "          # First we load existing data into a dict.\n",
    "        file_data = json.load(file)\n",
    "        # Join new_data with file_data inside emp_details\n",
    "        file_data[\"wizard_ergebnisse\"].append(new_data)\n",
    "        # Sets file's current position at offset.\n",
    "        file.seek(0)\n",
    "        # convert back to json.\n",
    "        json.dump(file_data, file, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generierung von Antworten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "counter=1\n",
    "\n",
    "result_data = {\n",
    "    \"ID\":\"\",\n",
    "    \"frage\": \"\",\n",
    "    \"ref_antwort\": \"\",\n",
    "    \"RAG_kontex1\":\"\",\n",
    "    \"RAG_kontex2\":\"\",\n",
    "    \"RAG_kontex3\":\"\",\n",
    "    \"RAG_wizard_antwort\": \"\",\n",
    "    \"Pure_LLM_wizard_antwort\":\"\"\n",
    "}\n",
    "for question_answer in json_data:\n",
    "    question = question_answer.get('frage', '')\n",
    "    id=question_answer.get('ID','')\n",
    "    print(f\"{question}####################################################### {counter}\")\n",
    "    ref_ant = question_answer.get('ref_antwort', '')\n",
    "\n",
    "\n",
    "    print(f\"RAG Start Frage {counter}-------------------------------------------------------------------------\")\n",
    "    RAG_model_ant= qa_RAG_chain(question)# generiert RAG-Antwort auf Fragen\n",
    "    print(f\"RAG Fnish Frage {counter}-------------------------------------------------------------------------\")\n",
    "\n",
    "    \n",
    "    print(f\"Pure_LLM Start Frage {counter}-------------------------------------------------------------------------\")\n",
    "    pure_llm_antwort=llm_chain.run(question)# generiert Pure-LLM-Antwort auf Fragen\n",
    "    print(f\"Pure_LLM Fnish Frage {counter}-------------------------------------------------------------------------\")\n",
    "    \n",
    "    kontext1=RAG_model_ant[\"source_documents\"][0].page_content\n",
    "    kontext2=RAG_model_ant[\"source_documents\"][1].page_content\n",
    "    kontext3=RAG_model_ant[\"source_documents\"][2].page_content\n",
    "    result_data[\"ID\"]=id\n",
    "    result_data['frage']=question\n",
    "    result_data['ref_antwort']=ref_ant\n",
    "    result_data['RAG_kontex1']=kontext1\n",
    "    result_data['RAG_kontex2']=kontext2\n",
    "    result_data['RAG_kontex3']=kontext3\n",
    "    \n",
    "    result_data['RAG_wizard_antwort']=RAG_model_ant[\"result\"]\n",
    "    result_data['Pure_LLM_wizard_antwort']=pure_llm_antwort\n",
    "    write_json(result_data)# Funktion aufruf zur Speicherung der generierten RAG und Pure-LLM Antworten\n",
    "    counter+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
